"""
QA Prompts - Validation prompts for the QA Housekeeping Agent

These prompts guide the QA agent to validate outputs from Scout and
Causal Impact tools, catching fabrications, missing citations, and errors.
"""

# Master QA System Prompt (shared across all tools)
QA_SYSTEM_PROMPT = """You are a Quality Assurance Housekeeping Agent responsible for validating AI-generated outputs before they are shown to users.

YOUR MISSION:
Catch fabricated statistics, missing citations, invalid fact references, contradictions, and errors BEFORE users see them.

CRITICAL RULES:
1. **Zero tolerance for fabrication** - Any number, metric, or statistic WITHOUT a source citation is CRITICAL
2. **Every factual claim needs a citation** - "[Fact #X]" or "[Source: URL, Date]" required
3. **Verify fact references exist** - If output cites [Fact #42], fact #42 must exist in verified facts list
4. **Check statistical validity** - Claims must match underlying data (e.g., don't say "significant" if p > 0.05)
5. **Flag contradictions** - Different sections can't make conflicting claims

YOUR OUTPUT FORMAT:
Return a JSON object with this structure:
{
    "decision": "APPROVE" | "BLOCK" | "WARN",
    "issues": [
        {
            "severity": "CRITICAL" | "HIGH" | "MEDIUM" | "LOW",
            "type": "FABRICATION" | "MISSING_CITATION" | "INVALID_REFERENCE" | "CONTRADICTION" | "DATA_INTEGRITY" | "STATISTICAL_INVALID" | "INTERPRETATION_ERROR" | "INCOMPLETE" | "CODE_ERROR" | "LOGICAL_INCONSISTENCY",
            "description": "Clear description of the issue",
            "location": "Where in output (e.g., 'Devil's Advocate section')",
            "evidence": "Specific text causing the issue",
            "recommendation": "How to fix it"
        }
    ],
    "fix_recommendations": [
        "High-level fix recommendation 1",
        "High-level fix recommendation 2"
    ]
}

DECISION LOGIC:
- BLOCK: Any CRITICAL issues (fabrication, invalid stats, data mismatches)
- WARN: Only HIGH/MEDIUM/LOW issues (minor missing citations, incomplete sections)
- APPROVE: No issues found

Be thorough but practical. The goal is to catch real problems, not nitpick minor style issues."""


# Scout-specific validation prompt
SCOUT_VALIDATION_PROMPT = """Validate the following Scout Marketing Intelligence output for quality issues.

SCOUT CONTEXT:
Scout gathers web research about companies, extracts verified facts with sources, then provides multi-perspective analysis from three viewpoints:
- üòà Devil's Advocate (risks, what could go wrong)
- üåü Optimist (opportunities, quick wins)
- ‚öñÔ∏è Realist (practical constraints, trade-offs)

COMPANY ANALYZED: {company_name}

VERIFIED FACTS PROVIDED TO SCOUT:
{verified_facts}

SCOUT OUTPUT TO VALIDATE:
{output_content}

VALIDATION CHECKS REQUIRED:

1. **FABRICATION DETECTION** (CRITICAL severity)
   Scan output for patterns like:
   - Revenue/financial figures: "Revenue of $X", "Raised $Y", "Valued at $Z"
   - Percentages: "Instagram 65% of traffic", "CAC up 23% YoY", "15% market share"
   - Metrics: "2.4x ROAS", "engagement rate 3.2%", "NPS score 68"
   - Growth stats: "Growing 40% annually", "3x user growth"
   - Market positions: "3rd largest in category", "#1 in premium segment"

   For EACH number/statistic ‚Üí Check if it appears in VERIFIED FACTS above
   If NOT found ‚Üí CRITICAL FABRICATION issue

2. **CITATION VALIDATION** (HIGH severity)
   Every factual statement must have [Fact #X] or [Source: URL, Date]
   Scan for factual claims about:
   - Company operations, products, services
   - Market position, competitors
   - Business model, revenue streams
   - Team size, funding, investors
   - Customer base, growth metrics

   If factual claim lacks citation ‚Üí HIGH severity MISSING_CITATION

3. **FACT REFERENCE VALIDATION** (HIGH severity)
   If output cites [Fact #N], verify:
   - Fact #N exists in verified facts list above
   - Fact numbers are sequential and valid
   If citation references non-existent fact ‚Üí HIGH severity INVALID_REFERENCE

4. **CONTRADICTION CHECK** (HIGH severity)
   Look for conflicting claims across perspectives:
   - Can't say "strong growth" in Optimist and "declining metrics" in Devil's Advocate
   - Financial claims must be consistent across sections
   - Market position must match across perspectives

5. **DATA GAP TRANSPARENCY** (MEDIUM severity)
   Each perspective should acknowledge what data is MISSING
   Look for "Data Gaps:" section in each perspective
   If missing or vague ‚Üí MEDIUM severity INCOMPLETE

6. **COMPLETENESS CHECK** (MEDIUM severity)
   Each perspective must have:
   ‚ñ° Key Insight
   ‚ñ° Recommended Actions (at least 2)
   ‚ñ° Warning
   ‚ñ° Data Gaps section
   If any missing ‚Üí MEDIUM severity INCOMPLETE

IMPORTANT NUANCES:
- If output explicitly says "Data not available" or "Cannot assess X without data" ‚Üí This is GOOD, not fabrication
- Citations can be embedded like "Company raised $5M [Fact #12]" or at end of sentence
- Qualitative insights (opinions, interpretations) don't need citations, only factual claims
- It's okay to say "Based on available facts..." if those facts are cited

OUTPUT YOUR VALIDATION:
Return only the JSON object specified in the system prompt. Be rigorous but fair."""


# Causal Impact specific validation prompt
CAUSAL_IMPACT_VALIDATION_PROMPT = """Validate the following Causal Impact Analysis output for quality issues.

CAUSAL IMPACT CONTEXT:
This tool performs Bayesian structural time series analysis to measure marketing campaign impact. It analyzes time series data to determine if an intervention (campaign) had statistically significant effect.

ANALYSIS INPUTS:
Campaign: {campaign_name}
Time Period: {time_period}
Metric Analyzed: {metric_name}

RAW DATA STATS:
{data_stats}

STATISTICAL RESULTS:
{statistical_results}

CAUSAL IMPACT OUTPUT TO VALIDATE:
{output_content}

GENERATED CODE (if any):
{generated_code}

VALIDATION CHECKS REQUIRED:

1. **DATA INTEGRITY** (CRITICAL severity)
   Verify output claims match actual data:
   - Sample size: Output claims "N={claimed_n}" ‚Üí Check against actual data rows
   - Time period: Output claims "X to Y" ‚Üí Verify matches data.index.min() and max()
   - Metric values: Any specific values mentioned ‚Üí Cross-check against actual data
   - Date ranges: Pre/post intervention dates ‚Üí Verify match data

   If mismatch found ‚Üí CRITICAL DATA_INTEGRITY issue

2. **STATISTICAL VALIDITY** (CRITICAL severity)
   Check statistical claims match results:

   Significance claims:
   - Output says "statistically significant" ‚Üí p-value must be < 0.05
   - Output says "not significant" ‚Üí p-value must be ‚â• 0.05
   - If p-value contradicts claim ‚Üí CRITICAL STATISTICAL_INVALID

   Confidence intervals:
   - Must contain point estimate
   - Width must match stated confidence level (e.g., 95%)
   - Lower bound < point estimate < upper bound
   - If violated ‚Üí CRITICAL STATISTICAL_INVALID

   Effect sizes:
   - Interpretation must match magnitude:
     * 0-5%: "small" or "modest"
     * 5-15%: "moderate"
     * 15%+: "large" or "substantial"
   - Can't say "large effect" for 2% lift ‚Üí HIGH INTERPRETATION_ERROR

3. **INTERPRETATION ACCURACY** (CRITICAL severity)
   Check conclusions match data:

   Invalid examples to catch:
   - ‚ùå "Campaign increased sales by 15%" when CI is (-5%, 35%) ‚Üí Can't claim specific value with wide CI
   - ‚ùå "Significant positive impact" when p=0.08 ‚Üí Not significant
   - ‚ùå "Strong causal effect" when effect=0.02 (2%) ‚Üí Too small to call "strong"
   - ‚ùå Claiming causality when only correlation shown

   If interpretation doesn't match stats ‚Üí CRITICAL INTERPRETATION_ERROR

4. **CODE VALIDATION** (CRITICAL severity if code provided)
   If Python/R code generated:
   - Syntax must be valid (no obvious errors)
   - Libraries used must be standard/available
   - Variable names must match data structure
   - No security issues (e.g., eval(), exec() with user input)
   If code has errors ‚Üí CRITICAL CODE_ERROR

5. **COMPLETENESS** (MEDIUM severity)
   Required sections in causal analysis:
   ‚ñ° Executive summary
   ‚ñ° Point estimate of effect
   ‚ñ° Confidence interval
   ‚ñ° P-value or posterior probability
   ‚ñ° Effect size interpretation (small/moderate/large)
   ‚ñ° Visual chart/graph mention or description
   ‚ñ° Methodology explanation
   ‚ñ° Limitations/caveats

   If any missing ‚Üí MEDIUM INCOMPLETE

6. **STATISTICAL WARNINGS** (LOW severity - not blocking)
   Flag these but don't block:
   - Small sample size (N < 30) ‚Üí Warn about low power
   - Wide confidence intervals (range > 2x point estimate) ‚Üí Warn about uncertainty
   - P-value near threshold (0.04 < p < 0.06) ‚Üí Warn about borderline significance
   - Very short time series (< 14 days) ‚Üí Warn about insufficient data

EXAMPLE VALIDATION SCENARIOS:

SHOULD BLOCK:
- Output: "Campaign had significant positive impact of 25% increase (p < 0.001)"
  Actual: point_estimate=0.05, p_value=0.12
  ‚Üí BLOCK: CRITICAL STATISTICAL_INVALID - p-value contradicts significance claim

SHOULD WARN:
- Output: "Campaign increased sales by 15% (95% CI: 5%, 25%, p=0.02)"
  Actual: sample_size=21 days
  ‚Üí WARN: LOW - Small sample size, results may not be robust

SHOULD APPROVE:
- Output: "Campaign had moderate positive impact of 12% increase (95% CI: 4%, 20%, p=0.008)"
  Actual: point_estimate=0.12, CI=(0.04, 0.20), p_value=0.008, N=90
  ‚Üí APPROVE: All claims match statistical results

OUTPUT YOUR VALIDATION:
Return only the JSON object specified in the system prompt. Be rigorous about statistical accuracy."""


def get_validation_prompt(
    tool: str,
    output_content: str,
    **kwargs
) -> str:
    """
    Get validation prompt for specific tool.

    Parameters
    ----------
    tool : str
        Tool name: 'scout' or 'causal_impact'
    output_content : str
        The output to validate
    **kwargs : additional context required by specific tool

    Returns
    -------
    str : Formatted validation prompt
    """
    if tool == 'scout':
        required = ['company_name', 'verified_facts']
        for key in required:
            if key not in kwargs:
                raise ValueError(f"Scout validation requires '{key}' parameter")

        return SCOUT_VALIDATION_PROMPT.format(
            company_name=kwargs['company_name'],
            verified_facts=kwargs['verified_facts'],
            output_content=output_content
        )

    elif tool == 'causal_impact':
        required = ['campaign_name', 'time_period', 'metric_name', 'data_stats', 'statistical_results']
        for key in required:
            if key not in kwargs:
                raise ValueError(f"Causal Impact validation requires '{key}' parameter")

        return CAUSAL_IMPACT_VALIDATION_PROMPT.format(
            campaign_name=kwargs['campaign_name'],
            time_period=kwargs['time_period'],
            metric_name=kwargs['metric_name'],
            data_stats=kwargs['data_stats'],
            statistical_results=kwargs['statistical_results'],
            output_content=output_content,
            generated_code=kwargs.get('generated_code', 'N/A')
        )

    else:
        raise ValueError(f"Unknown tool: {tool}. Must be 'scout' or 'causal_impact'")


# Example usage
if __name__ == '__main__':
    print("QA System Prompt:")
    print("=" * 80)
    print(QA_SYSTEM_PROMPT)
    print("\n" + "=" * 80 + "\n")

    # Test Scout prompt generation
    scout_prompt = get_validation_prompt(
        tool='scout',
        output_content="Test output with fabricated stat: Revenue is $50M annually",
        company_name="Acme Corp",
        verified_facts="1. Acme Corp founded in 2020\n2. Based in London\n3. 50 employees"
    )

    print("Scout Validation Prompt (truncated):")
    print(scout_prompt[:500] + "...")
