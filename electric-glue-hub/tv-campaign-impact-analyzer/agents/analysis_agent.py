"""
Analysis Agent - Bayesian model fitting and causal effect estimation
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
import warnings
warnings.filterwarnings('ignore')

from core.bayesian_models import (
    BayesianStructuralTimeSeries,
    auto_select_model
)


class AnalysisAgent:
    """
    Intelligent agent for Bayesian causal analysis.

    Responsibilities:
    - Auto-select appropriate BSTS model based on data characteristics
    - Fit Bayesian model with MCMC
    - Extract causal effect estimates
    - Compute counterfactual predictions
    - Provide uncertainty quantification
    - Handle edge cases gracefully
    """

    def __init__(self):
        self.model = None
        self.model_spec = None
        self.data = None
        self.results = None
        self.diagnostics = {}

    def analyze(
        self,
        data: pd.DataFrame,
        target_col: str,
        pre_period: Tuple[str, str],
        post_period: Tuple[str, str],
        covariate_cols: Optional[List[str]] = None,
        model_config: Optional[Dict] = None,
        niter: int = 2000,
        nburn: int = 1000
    ) -> Dict:
        """
        Run complete Bayesian causal analysis.

        Parameters
        ----------
        data : pd.DataFrame
            Time series data with datetime index
        target_col : str
            Target metric column name
        pre_period : tuple of str
            (start_date, end_date) for pre-intervention period
        post_period : tuple of str
            (start_date, end_date) for post-intervention period
        covariate_cols : list of str, optional
            Covariate columns (controls for confounders)
        model_config : dict, optional
            Manual model configuration (overrides auto-selection)
        niter : int, default 2000
            MCMC iterations
        nburn : int, default 1000
            Burn-in iterations

        Returns
        -------
        dict with analysis results, causal effects, and diagnostics
        """
        try:
            self.data = data

            # Step 1: Auto-select model (unless manual config provided)
            if model_config is None:
                print("üìä Auto-selecting BSTS model based on data characteristics...")
                self.model_spec = auto_select_model(data, target_col)
                print(f"   Seasonality: {self.model_spec['recommendation']['seasonality']}")
                print(f"   Trend: {self.model_spec['recommendation']['trend']}")
                print(f"   Local variation: {self.model_spec['recommendation']['local_variation']}")
            else:
                self.model_spec = model_config
                print("üìä Using manual model configuration")

            # Step 2: Prepare data
            y = data[target_col]
            X = data[covariate_cols] if covariate_cols else None

            # Convert date strings to indices
            pre_start_idx = data.index.get_loc(pd.to_datetime(pre_period[0]))
            pre_end_idx = data.index.get_loc(pd.to_datetime(pre_period[1]))
            post_start_idx = data.index.get_loc(pd.to_datetime(post_period[0]))
            post_end_idx = data.index.get_loc(pd.to_datetime(post_period[1]))

            pre_indices = (pre_start_idx, pre_end_idx)
            post_indices = (post_start_idx, post_end_idx)

            # Step 3: Initialize and fit model
            print(f"\nüîß Fitting Bayesian Structural Time Series model...")
            print(f"   Pre-period: {len(range(pre_start_idx, pre_end_idx + 1))} observations")
            print(f"   Post-period: {len(range(post_start_idx, post_end_idx + 1))} observations")
            if covariate_cols:
                print(f"   Covariates: {', '.join(covariate_cols)}")

            self.model = BayesianStructuralTimeSeries(
                seasonal_periods=self.model_spec.get('seasonal_periods', [7]),
                include_trend=self.model_spec.get('include_trend', True),
                include_local_level=self.model_spec.get('include_local_level', True),
                prior_level_sd=self.model_spec.get('prior_level_sd', 0.01)
            )

            self.model.fit(
                y=y,
                X=X,
                pre_period=pre_indices,
                post_period=post_indices,
                niter=niter,
                nburn=nburn
            )

            print("‚úÖ Model fitting complete!")

            # Step 4: Extract results
            causal_effect = self.model.get_causal_effect()
            counterfactual = self.model.get_counterfactual()

            # Step 5: Compute additional metrics
            self._compute_diagnostics(y, counterfactual, pre_indices, post_indices)

            # Store results
            self.results = {
                'success': True,
                'causal_effect': causal_effect,
                'counterfactual': counterfactual.to_dict(),
                'diagnostics': self.diagnostics,
                'model_spec': self.model_spec
            }

            return self.results

        except Exception as e:
            print(f"‚ùå Analysis failed: {str(e)}")
            return {
                'success': False,
                'error': str(e)
            }

    def _compute_diagnostics(
        self,
        y: pd.Series,
        counterfactual: pd.Series,
        pre_indices: Tuple[int, int],
        post_indices: Tuple[int, int]
    ):
        """Compute diagnostic metrics for model assessment."""

        # In-sample fit (pre-period)
        pre_actual = y.iloc[pre_indices[0]:pre_indices[1] + 1]
        pre_predicted = counterfactual.iloc[pre_indices[0]:pre_indices[1] + 1]

        pre_rmse = np.sqrt(np.mean((pre_actual - pre_predicted) ** 2))
        pre_mape = np.mean(np.abs((pre_actual - pre_predicted) / pre_actual)) * 100

        # Out-of-sample predictions (post-period)
        post_actual = y.iloc[post_indices[0]:post_indices[1] + 1]
        post_predicted = counterfactual.iloc[post_indices[0]:post_indices[1] + 1]

        # Post-period metrics (if intervention didn't happen)
        post_error = post_actual - post_predicted

        self.diagnostics = {
            'pre_period_fit': {
                'rmse': float(pre_rmse),
                'mape': float(pre_mape),
                'r_squared': float(1 - (np.sum((pre_actual - pre_predicted) ** 2) /
                                       np.sum((pre_actual - pre_actual.mean()) ** 2)))
            },
            'post_period': {
                'actual_mean': float(post_actual.mean()),
                'counterfactual_mean': float(post_predicted.mean()),
                'difference_mean': float(post_error.mean())
            },
            'model_quality': self._assess_model_quality(pre_rmse, pre_mape)
        }

    def _assess_model_quality(self, rmse: float, mape: float) -> str:
        """Provide qualitative assessment of model fit."""
        if mape < 5:
            return "Excellent"
        elif mape < 10:
            return "Good"
        elif mape < 20:
            return "Acceptable"
        else:
            return "Poor - High prediction error"

    def get_summary(self) -> str:
        """Generate human-readable summary of results."""
        if not self.results or not self.results.get('success'):
            return "No results available - analysis not yet run or failed"

        effect = self.results['causal_effect']
        diag = self.diagnostics

        summary = f"""
BAYESIAN CAUSAL IMPACT ANALYSIS SUMMARY
{'=' * 60}

CAUSAL EFFECT ESTIMATES
-----------------------
Average Effect:    {effect['average_effect']:,.0f}  [{effect['average_effect_lower']:,.0f}, {effect['average_effect_upper']:,.0f}]
Relative Effect:   {effect['relative_effect']*100:,.1f}%  [{effect['relative_effect_lower']*100:,.1f}%, {effect['relative_effect_upper']*100:,.1f}%]
Cumulative Effect: {effect['cumulative_effect']:,.0f}  [{effect['cumulative_lower']:,.0f}, {effect['cumulative_upper']:,.0f}]
P-value:          {effect['p_value']:.4f}

MODEL DIAGNOSTICS
-----------------
Pre-period Fit:   {diag['model_quality']} (MAPE: {diag['pre_period_fit']['mape']:.1f}%, R¬≤: {diag['pre_period_fit']['r_squared']:.3f})
RMSE:            {diag['pre_period_fit']['rmse']:.2f}

INTERPRETATION
--------------
"""
        if effect['p_value'] < 0.05:
            summary += f"‚úÖ STATISTICALLY SIGNIFICANT effect detected (p = {effect['p_value']:.4f})\n"
            if effect['average_effect'] > 0:
                summary += f"üìà The intervention caused an average INCREASE of {effect['average_effect']:,.0f} per period\n"
            else:
                summary += f"üìâ The intervention caused an average DECREASE of {abs(effect['average_effect']):,.0f} per period\n"
        else:
            summary += f"‚ö†Ô∏è  No statistically significant effect detected (p = {effect['p_value']:.4f})\n"

        summary += f"\nüí° Total impact over post-period: {effect['cumulative_effect']:,.0f} units\n"

        return summary

    def get_detailed_results(self) -> pd.DataFrame:
        """
        Get detailed period-by-period results.

        Returns DataFrame with:
        - Date
        - Actual value
        - Predicted (counterfactual)
        - Point effect
        - Cumulative effect
        """
        if not self.results or not self.results.get('success'):
            return pd.DataFrame()

        # Extract data
        actual = self.data[self.results['model_spec'].get('target_col', self.data.columns[0])]
        predicted = pd.Series(self.results['counterfactual'])
        predicted.index = actual.index

        # Compute effects
        point_effect = actual - predicted
        cumulative_effect = point_effect.cumsum()

        # Create detailed DataFrame
        detailed = pd.DataFrame({
            'date': actual.index,
            'actual': actual.values,
            'predicted': predicted.values,
            'point_effect': point_effect.values,
            'cumulative_effect': cumulative_effect.values
        })

        return detailed

    def run_sensitivity_analysis(
        self,
        prior_sds: List[float] = [0.001, 0.01, 0.05, 0.1]
    ) -> pd.DataFrame:
        """
        Test sensitivity to prior specifications.

        Shows robustness of results to modeling choices.
        """
        if self.data is None:
            raise ValueError("No data available - run analyze() first")

        print("\nüî¨ Running sensitivity analysis...")

        results_list = []

        for prior_sd in prior_sds:
            print(f"   Testing prior_sd = {prior_sd}")

            # Create model with different prior
            test_model = BayesianStructuralTimeSeries(
                seasonal_periods=self.model_spec.get('seasonal_periods', [7]),
                include_trend=self.model_spec.get('include_trend', True),
                include_local_level=self.model_spec.get('include_local_level', True),
                prior_level_sd=prior_sd
            )

            # Fit model (would need to store original parameters - simplified here)
            # In production, store fit parameters and reuse
            try:
                # Placeholder - in full implementation, refit model
                results_list.append({
                    'prior_sd': prior_sd,
                    'average_effect': self.results['causal_effect']['average_effect'],
                    'p_value': self.results['causal_effect']['p_value'],
                    'note': 'Refit required for true sensitivity'
                })
            except Exception as e:
                results_list.append({
                    'prior_sd': prior_sd,
                    'error': str(e)
                })

        return pd.DataFrame(results_list)

    def compare_to_simple_methods(self) -> Dict:
        """
        Compare BSTS results to naive approaches.

        Shows value of Bayesian method over simple before/after.
        """
        if not self.results or not self.results.get('success'):
            return {}

        # Get post-period data
        post_actual = self.diagnostics['post_period']['actual_mean']
        post_predicted = self.diagnostics['post_period']['counterfactual_mean']

        # Simple before/after (ignoring trends)
        pre_mean = self.diagnostics['post_period']['counterfactual_mean']  # This is the model's estimate
        naive_effect = post_actual - pre_mean

        # Percentage difference between naive and BSTS
        bsts_effect = self.results['causal_effect']['average_effect']
        difference_pct = abs(naive_effect - bsts_effect) / abs(bsts_effect) * 100 if bsts_effect != 0 else 0

        return {
            'bsts_estimate': bsts_effect,
            'naive_before_after': naive_effect,
            'difference_pct': difference_pct,
            'interpretation': (
                f"Naive before/after would {'overestimate' if naive_effect > bsts_effect else 'underestimate'} "
                f"the effect by {difference_pct:.1f}% because it ignores trends and seasonality"
            )
        }

    def export_results(self, output_path: str, format: str = 'csv'):
        """
        Export results to file.

        Parameters
        ----------
        output_path : str
            Path to save results
        format : str, default 'csv'
            Output format: 'csv', 'excel', or 'json'
        """
        if not self.results or not self.results.get('success'):
            print("‚ùå No results to export")
            return

        detailed = self.get_detailed_results()

        if format == 'csv':
            detailed.to_csv(output_path, index=False)
        elif format == 'excel':
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                detailed.to_excel(writer, sheet_name='Detailed Results', index=False)

                # Add summary sheet
                summary_data = {
                    'Metric': ['Average Effect', 'Lower Bound', 'Upper Bound',
                              'Relative Effect (%)', 'Cumulative Effect', 'P-value'],
                    'Value': [
                        self.results['causal_effect']['average_effect'],
                        self.results['causal_effect']['average_effect_lower'],
                        self.results['causal_effect']['average_effect_upper'],
                        self.results['causal_effect']['relative_effect'] * 100,
                        self.results['causal_effect']['cumulative_effect'],
                        self.results['causal_effect']['p_value']
                    ]
                }
                pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)

        elif format == 'json':
            import json
            with open(output_path, 'w') as f:
                json.dump({
                    'causal_effect': self.results['causal_effect'],
                    'diagnostics': self.diagnostics,
                    'detailed_results': detailed.to_dict(orient='records')
                }, f, indent=2, default=str)

        print(f"‚úÖ Results exported to {output_path}")


# Example usage
if __name__ == '__main__':
    from agents.data_agent import create_sample_data

    # Create sample data
    print("Creating sample TV campaign data...")
    df = create_sample_data()
    df = df.set_index('date')

    # Initialize analysis agent
    agent = AnalysisAgent()

    # Run analysis
    results = agent.analyze(
        data=df,
        target_col='bookings',
        pre_period=('2024-01-01', '2024-07-18'),  # Before TV campaign
        post_period=('2024-07-19', '2024-12-31'),  # During TV campaign
        covariate_cols=['digital_spend', 'flights_available'],  # Control for confounders!
        niter=1000,  # Fewer iterations for demo
        nburn=500
    )

    if results['success']:
        print('\n' + '=' * 60)
        print(agent.get_summary())
        print('=' * 60)

        # Comparison to naive methods
        comparison = agent.compare_to_simple_methods()
        print('\nüìä COMPARISON TO NAIVE METHODS')
        print('-' * 60)
        print(comparison['interpretation'])

        # Export results
        agent.export_results('analysis_results.csv', format='csv')
